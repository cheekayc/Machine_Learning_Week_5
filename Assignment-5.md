Assignment 5
================
Chee Kay Cheong
2023-02-14

``` r
knitr::opts_chunk$set(warning = FALSE, message = FALSE, set.seed(123))

library(tidyverse) 
library(caret) 
library(glmnet)
```

# Load and clean dataset, data partitioning

``` r
# Load and clean dataset
alcohol = read_csv("./Data/alcohol_use.csv") %>%  
  janitor::clean_names() %>% 
  select(-x1) %>% 
  mutate(
    alc_consumption = as_factor(alc_consumption),
    alc_consumption = relevel(alc_consumption, ref = "NotCurrentUse"))

# Check the distribution of the outcome
alcohol %>% 
  select(alc_consumption) %>% 
  group_by(alc_consumption) %>% 
  count()
```

    ## # A tibble: 2 × 2
    ## # Groups:   alc_consumption [2]
    ##   alc_consumption     n
    ##   <fct>           <int>
    ## 1 NotCurrentUse     881
    ## 2 CurrentUse       1004

``` r
# Quite balance...

# Partition data into 70/30 split
set.seed(123)

train_index = 
  alcohol$alc_consumption %>% createDataPartition(p = 0.7, list = F)

train_data = alcohol[train_index, ]
test_data = alcohol[-train_index, ]
```

# 1. Create 3 different models

### Elastic Net Model

``` r
set.seed(123)

# Set validation method and options
control.settings = trainControl(method = "repeatedcv", number = 10, repeats = 10)

# Fit model
EN_model = train(alc_consumption ~ ., data = train_data, method = "glmnet", trControl = control.settings, preProc = c("center", "scale"), tuneLength = 10)

# Find best tuned parameters
EN_model$bestTune
```

    ##    alpha   lambda
    ## 63   0.7 0.263144

``` r
EN_model$results
```

    ##    alpha       lambda  Accuracy     Kappa AccuracySD    KappaSD
    ## 1    0.1 0.0003244162 0.8021249 0.6025511 0.03294294 0.06579782
    ## 2    0.1 0.0007494434 0.8021249 0.6025511 0.03294294 0.06579782
    ## 3    0.1 0.0017313116 0.8024262 0.6031436 0.03287341 0.06564162
    ## 4    0.1 0.0039995543 0.8034851 0.6051458 0.03279451 0.06555265
    ## 5    0.1 0.0092394891 0.8039408 0.6058913 0.03328184 0.06665759
    ## 6    0.1 0.0213444181 0.8063617 0.6106158 0.03202195 0.06414912
    ## 7    0.1 0.0493083742 0.8046163 0.6067939 0.02988519 0.05994885
    ## 8    0.1 0.1139087396 0.8064414 0.6100533 0.02911429 0.05861242
    ## 9    0.1 0.2631439626 0.8121228 0.6206349 0.03013028 0.06082484
    ## 10   0.2 0.0003244162 0.8018965 0.6020975 0.03309423 0.06608941
    ## 11   0.2 0.0007494434 0.8018965 0.6020975 0.03309423 0.06608941
    ## 12   0.2 0.0017313116 0.8020486 0.6023948 0.03295346 0.06581179
    ## 13   0.2 0.0039995543 0.8031810 0.6045565 0.03225080 0.06445365
    ## 14   0.2 0.0092394891 0.8031804 0.6043968 0.03365681 0.06737383
    ## 15   0.2 0.0213444181 0.8046146 0.6071569 0.03196622 0.06393726
    ## 16   0.2 0.0493083742 0.8054497 0.6084910 0.03042692 0.06098375
    ## 17   0.2 0.1139087396 0.8138636 0.6248971 0.03094883 0.06221861
    ## 18   0.2 0.2631439626 0.8146212 0.6257956 0.03087902 0.06227813
    ## 19   0.3 0.0003244162 0.8018219 0.6019633 0.03295512 0.06582427
    ## 20   0.3 0.0007494434 0.8018219 0.6019633 0.03295512 0.06582427
    ## 21   0.3 0.0017313116 0.8018213 0.6019481 0.03300924 0.06593356
    ## 22   0.3 0.0039995543 0.8023487 0.6029174 0.03224802 0.06444693
    ## 23   0.3 0.0092394891 0.8024963 0.6030821 0.03315694 0.06630218
    ## 24   0.3 0.0213444181 0.8041589 0.6062482 0.03235905 0.06484113
    ## 25   0.3 0.0493083742 0.8074206 0.6124634 0.03044239 0.06096267
    ## 26   0.3 0.1139087396 0.8136398 0.6243526 0.03023156 0.06076110
    ## 27   0.3 0.2631439626 0.8228772 0.6416157 0.03257385 0.06558539
    ## 28   0.4 0.0003244162 0.8015189 0.6013726 0.03290790 0.06571643
    ## 29   0.4 0.0007494434 0.8015189 0.6013726 0.03290790 0.06571643
    ## 30   0.4 0.0017313116 0.8016704 0.6016654 0.03307352 0.06602660
    ## 31   0.4 0.0039995543 0.8016669 0.6015640 0.03250717 0.06498841
    ## 32   0.4 0.0092394891 0.8015871 0.6012976 0.03288059 0.06570968
    ## 33   0.4 0.0213444181 0.8037078 0.6053558 0.03223943 0.06454479
    ## 34   0.4 0.0493083742 0.8078694 0.6134205 0.03122310 0.06244204
    ## 35   0.4 0.1139087396 0.8180309 0.6328067 0.02995074 0.06018181
    ## 36   0.4 0.2631439626 0.8362224 0.6676939 0.02896326 0.05900502
    ## 37   0.5 0.0003244162 0.8012921 0.6009304 0.03286712 0.06562396
    ## 38   0.5 0.0007494434 0.8012921 0.6009304 0.03286712 0.06562396
    ## 39   0.5 0.0017313116 0.8013685 0.6010661 0.03301821 0.06592134
    ## 40   0.5 0.0039995543 0.8006838 0.5996148 0.03256201 0.06505412
    ## 41   0.5 0.0092394891 0.8009070 0.6000084 0.03324327 0.06634827
    ## 42   0.5 0.0213444181 0.8026529 0.6032610 0.03229618 0.06458488
    ## 43   0.5 0.0493083742 0.8060615 0.6098632 0.02934645 0.05863607
    ## 44   0.5 0.1139087396 0.8175724 0.6319429 0.02973537 0.05967925
    ## 45   0.5 0.2631439626 0.8437938 0.6822244 0.03067875 0.06251526
    ## 46   0.6 0.0003244162 0.8013696 0.6010683 0.03294207 0.06576579
    ## 47   0.6 0.0007494434 0.8013696 0.6010683 0.03294207 0.06576579
    ## 48   0.6 0.0017313116 0.8010660 0.6004683 0.03285863 0.06558513
    ## 49   0.6 0.0039995543 0.8003830 0.5990509 0.03271832 0.06530242
    ## 50   0.6 0.0092394891 0.8006069 0.5994173 0.03266724 0.06515804
    ## 51   0.6 0.0213444181 0.8016675 0.6013774 0.03194833 0.06381970
    ## 52   0.6 0.0493083742 0.8045446 0.6069954 0.02909267 0.05814995
    ## 53   0.6 0.1139087396 0.8100034 0.6173658 0.03000637 0.05997214
    ## 54   0.6 0.2631439626 0.8609154 0.7153953 0.02471239 0.05136985
    ## 55   0.7 0.0003244162 0.8013702 0.6010715 0.03274571 0.06535043
    ## 56   0.7 0.0007494434 0.8013702 0.6010715 0.03274571 0.06535043
    ## 57   0.7 0.0017313116 0.8009891 0.6003139 0.03303027 0.06590708
    ## 58   0.7 0.0039995543 0.8001563 0.5986054 0.03272224 0.06533459
    ## 59   0.7 0.0092394891 0.7997729 0.5977894 0.03237309 0.06456908
    ## 60   0.7 0.0213444181 0.8005288 0.5992644 0.03163957 0.06317475
    ## 61   0.7 0.0493083742 0.8043214 0.6066148 0.02950123 0.05904294
    ## 62   0.7 0.1139087396 0.8065138 0.6107999 0.02927971 0.05861940
    ## 63   0.7 0.2631439626 0.8613717 0.7162831 0.02410804 0.05021350
    ## 64   0.8 0.0003244162 0.8011435 0.6006259 0.03258017 0.06502425
    ## 65   0.8 0.0007494434 0.8011435 0.6006259 0.03258017 0.06502425
    ## 66   0.8 0.0017313116 0.8001575 0.5986653 0.03266576 0.06514220
    ## 67   0.8 0.0039995543 0.7993982 0.5971088 0.03268451 0.06524277
    ## 68   0.8 0.0092394891 0.7992426 0.5967751 0.03240913 0.06466398
    ## 69   0.8 0.0213444181 0.7993936 0.5971428 0.03161516 0.06312098
    ## 70   0.8 0.0493083742 0.8024205 0.6029542 0.03012439 0.06013293
    ## 71   0.8 0.1139087396 0.8003727 0.5989898 0.03091001 0.06162219
    ## 72   0.8 0.2631439626 0.8613717 0.7162831 0.02410804 0.05021350
    ## 73   0.9 0.0003244162 0.8010672 0.6004778 0.03264234 0.06514239
    ## 74   0.9 0.0007494434 0.8010672 0.6004778 0.03264234 0.06514239
    ## 75   0.9 0.0017313116 0.8001575 0.5986629 0.03259470 0.06501487
    ## 76   0.9 0.0039995543 0.7996260 0.5975769 0.03237435 0.06461207
    ## 77   0.9 0.0092394891 0.7981097 0.5945942 0.03271839 0.06521876
    ## 78   0.9 0.0213444181 0.7984122 0.5953858 0.03145877 0.06274537
    ## 79   0.9 0.0493083742 0.7921981 0.5831468 0.03141914 0.06261120
    ## 80   0.9 0.1139087396 0.7730270 0.5468654 0.03188777 0.06308280
    ## 81   0.9 0.2631439626 0.8613717 0.7162831 0.02410804 0.05021350
    ## 82   1.0 0.0003244162 0.8008405 0.6000303 0.03266593 0.06515377
    ## 83   1.0 0.0007494434 0.8008405 0.6000303 0.03266593 0.06515377
    ## 84   1.0 0.0017313116 0.8005363 0.5994167 0.03256193 0.06498019
    ## 85   1.0 0.0039995543 0.7995508 0.5974373 0.03222499 0.06431934
    ## 86   1.0 0.0092394891 0.7978853 0.5942345 0.03334336 0.06643227
    ## 87   1.0 0.0213444181 0.7972798 0.5932744 0.03138683 0.06255130
    ## 88   1.0 0.0493083742 0.7837860 0.5668913 0.03336588 0.06629938
    ## 89   1.0 0.1139087396 0.7719681 0.5450556 0.03120473 0.06186270
    ## 90   1.0 0.2631439626 0.7091181 0.3917838 0.05416388 0.11619850

``` r
# Highest accuracy = 0.8515189, Kappa = 0.6956916, alpha = 0.7, lambda = 0.2578427450
```

Professor’s solution:

``` r
set.seed(123)

# Create vectors of lambda and alpha
lambda = 10^seq(-3, 3, length = 100)
alpha = 0.1*seq(1, 10, length = 10)

# Create Model 1 using `Caret`
model.1 = train(alc_consumption ~ ., data = train_data, method = "glmnet", trControl = trainControl("cv", number = 10), preProcess = c("center", "scale"), tuneGrid = expand.grid(alpha = alpha, lambda = lambda))

# Output best values of alpha & lambda
model.1$finalModel$tuneValue
```

    ##     alpha    lambda
    ## 541   0.6 0.2656088

``` r
model.1$bestTune
```

    ##     alpha    lambda
    ## 541   0.6 0.2656088

``` r
# Can store it if we want to use in a different implementation
best.alpha = model.1$bestTune$alpha
best.lambda = model.1$bestTune$lambda

# To view coefficients, must specify value of lambda
coef(model.1$finalModel, model.1$bestTune$lambda)
```

    ## 8 x 1 sparse Matrix of class "dgCMatrix"
    ##                                 s1
    ## (Intercept)             0.13658767
    ## neurotocism_score       .         
    ## extroversion_score      .         
    ## openness_score          .         
    ## agreeableness_score     .         
    ## conscientiousness_score .         
    ## impulsiveness_score     0.41353840
    ## sens_seeking_score      0.01148506

``` r
model.1$results[which.max(model.1$results$Accuracy), ] # find where the maximum accuracy is and print that row out.
```

    ##     alpha    lambda  Accuracy     Kappa AccuracySD    KappaSD
    ## 541   0.6 0.2656088 0.8621812 0.7180342 0.02533697 0.05243557

``` r
# OR

max(model.1$results$Accuracy)
```

    ## [1] 0.8621812

``` r
# OR

confusionMatrix(model.1)
```

    ## Cross-Validated (10 fold) Confusion Matrix 
    ## 
    ## (entries are percentual average cell counts across resamples)
    ##  
    ##                Reference
    ## Prediction      NotCurrentUse CurrentUse
    ##   NotCurrentUse          33.0        0.0
    ##   CurrentUse             13.8       53.3
    ##                             
    ##  Accuracy (average) : 0.8621

How to interpret ConfusionMatrix? 53.3% of the data is correctly
classified as those who are “CurrentUse” has been predicted as
“CurrentUse”. 33 % of the data is correctly classified as those who are
“NotCurrentUse” has been predicted as “NotCurrentUse”. 13.8% of the data
has been **misclassified** because 13.8% of the “NotCurrentUse” has been
predicted as “CurrentUse”.

### Traditional Logistic Regression

**If we still want to use `glmnet` we must set alpha and lambda equal to
0, so that it gives us a traditional logistic regression.**

``` r
set.seed(124)

# Set validation method and options
control.settings = trainControl(method = "repeatedcv", number = 10, repeats = 10)

# Fit model
logreg = train(alc_consumption ~ ., data = train_data, method = "glmnet", family = 'binomial', trControl = control.settings, preProc = c("center", "scale"), tuneLength = 10) 

# Find best tuned parameters
logreg$bestTune
```

    ##    alpha   lambda
    ## 63   0.7 0.263144

``` r
logreg$results
```

    ##    alpha       lambda  Accuracy     Kappa AccuracySD    KappaSD
    ## 1    0.1 0.0003244162 0.8033890 0.6050107 0.03462766 0.06940962
    ## 2    0.1 0.0007494434 0.8033890 0.6050107 0.03462766 0.06940962
    ## 3    0.1 0.0017313116 0.8034682 0.6051388 0.03476028 0.06969319
    ## 4    0.1 0.0039995543 0.8037713 0.6056690 0.03487375 0.06989411
    ## 5    0.1 0.0092394891 0.8052158 0.6084067 0.03434727 0.06887682
    ## 6    0.1 0.0213444181 0.8054431 0.6087180 0.03264459 0.06552677
    ## 7    0.1 0.0493083742 0.8055183 0.6086140 0.03515421 0.07066556
    ## 8    0.1 0.1139087396 0.8068125 0.6107947 0.03351113 0.06745733
    ## 9    0.1 0.2631439626 0.8119062 0.6202028 0.03122229 0.06311473
    ## 10   0.2 0.0003244162 0.8031623 0.6045692 0.03468296 0.06950629
    ## 11   0.2 0.0007494434 0.8031623 0.6045692 0.03468296 0.06950629
    ## 12   0.2 0.0017313116 0.8031652 0.6045377 0.03499549 0.07016187
    ## 13   0.2 0.0039995543 0.8036174 0.6053884 0.03533710 0.07082472
    ## 14   0.2 0.0092394891 0.8046075 0.6072110 0.03424341 0.06868635
    ## 15   0.2 0.0213444181 0.8051424 0.6081987 0.03264956 0.06555223
    ## 16   0.2 0.0493083742 0.8043039 0.6062132 0.03380366 0.06796297
    ## 17   0.2 0.1139087396 0.8143133 0.6257612 0.03385903 0.06823486
    ## 18   0.2 0.2631439626 0.8141647 0.6248613 0.03238930 0.06555241
    ## 19   0.3 0.0003244162 0.8032387 0.6047359 0.03490515 0.06994174
    ## 20   0.3 0.0007494434 0.8032387 0.6047359 0.03490515 0.06994174
    ## 21   0.3 0.0017313116 0.8031646 0.6045576 0.03493489 0.07001537
    ## 22   0.3 0.0039995543 0.8031618 0.6044875 0.03503723 0.07023206
    ## 23   0.3 0.0092394891 0.8042253 0.6065039 0.03491847 0.06999164
    ## 24   0.3 0.0213444181 0.8045329 0.6069926 0.03249296 0.06524831
    ## 25   0.3 0.0493083742 0.8069630 0.6115090 0.03309218 0.06649807
    ## 26   0.3 0.1139087396 0.8137830 0.6246216 0.03197847 0.06449196
    ## 27   0.3 0.2631439626 0.8244633 0.6446839 0.03198852 0.06475973
    ## 28   0.4 0.0003244162 0.8027112 0.6036975 0.03501023 0.07014612
    ## 29   0.4 0.0007494434 0.8027112 0.6036975 0.03501023 0.07014612
    ## 30   0.4 0.0017313116 0.8027112 0.6036820 0.03492683 0.06997731
    ## 31   0.4 0.0039995543 0.8024811 0.6031339 0.03500267 0.07016345
    ## 32   0.4 0.0092394891 0.8031646 0.6043998 0.03520864 0.07058574
    ## 33   0.4 0.0213444181 0.8037736 0.6055202 0.03256646 0.06539616
    ## 34   0.4 0.0493083742 0.8070347 0.6117558 0.03354291 0.06738168
    ## 35   0.4 0.1139087396 0.8166647 0.6300336 0.03262495 0.06589150
    ## 36   0.4 0.2631439626 0.8360615 0.6673074 0.02874478 0.05890540
    ## 37   0.5 0.0003244162 0.8025591 0.6033927 0.03514516 0.07041586
    ## 38   0.5 0.0007494434 0.8025591 0.6033927 0.03514516 0.07041586
    ## 39   0.5 0.0017313116 0.8025597 0.6033693 0.03524084 0.07061210
    ## 40   0.5 0.0039995543 0.8019531 0.6021046 0.03528164 0.07069076
    ## 41   0.5 0.0092394891 0.8015725 0.6012644 0.03507915 0.07028016
    ## 42   0.5 0.0213444181 0.8016535 0.6012712 0.03343278 0.06709383
    ## 43   0.5 0.0493083742 0.8053617 0.6083754 0.03319475 0.06671279
    ## 44   0.5 0.1139087396 0.8162819 0.6293215 0.03181391 0.06420011
    ## 45   0.5 0.2631439626 0.8440949 0.6828403 0.02777262 0.05691301
    ## 46   0.6 0.0003244162 0.8024839 0.6032413 0.03518261 0.07046939
    ## 47   0.6 0.0007494434 0.8024839 0.6032413 0.03518261 0.07046939
    ## 48   0.6 0.0017313116 0.8022567 0.6027581 0.03510569 0.07035099
    ## 49   0.6 0.0039995543 0.8014216 0.6010568 0.03555574 0.07125315
    ## 50   0.6 0.0092394891 0.7999822 0.5981379 0.03580005 0.07171514
    ## 51   0.6 0.0213444181 0.8020340 0.6020979 0.03453570 0.06921207
    ## 52   0.6 0.0493083742 0.8033219 0.6045058 0.03424698 0.06874098
    ## 53   0.6 0.1139087396 0.8105236 0.6184242 0.03239882 0.06495485
    ## 54   0.6 0.2631439626 0.8602346 0.7139496 0.02589942 0.05397156
    ## 55   0.7 0.0003244162 0.8024076 0.6030811 0.03525983 0.07064423
    ## 56   0.7 0.0007494434 0.8024076 0.6030811 0.03525983 0.07064423
    ## 57   0.7 0.0017313116 0.8019536 0.6021348 0.03529804 0.07074327
    ## 58   0.7 0.0039995543 0.8007403 0.5997003 0.03599386 0.07212892
    ## 59   0.7 0.0092394891 0.8000608 0.5983418 0.03587346 0.07187993
    ## 60   0.7 0.0213444181 0.8015020 0.6011523 0.03527379 0.07062458
    ## 61   0.7 0.0493083742 0.8032496 0.6045033 0.03338774 0.06699943
    ## 62   0.7 0.1139087396 0.8065864 0.6109806 0.03320800 0.06663995
    ## 63   0.7 0.2631439626 0.8613681 0.7162082 0.02625337 0.05470655
    ## 64   0.8 0.0003244162 0.8021798 0.6026348 0.03523452 0.07058911
    ## 65   0.8 0.0007494434 0.8021798 0.6026348 0.03523452 0.07058911
    ## 66   0.8 0.0017313116 0.8014233 0.6010874 0.03534915 0.07080887
    ## 67   0.8 0.0039995543 0.8005153 0.5992439 0.03582773 0.07178860
    ## 68   0.8 0.0092394891 0.7993807 0.5970361 0.03536952 0.07084354
    ## 69   0.8 0.0213444181 0.8002100 0.5987505 0.03545106 0.07092882
    ## 70   0.8 0.0493083742 0.8003712 0.5989466 0.03389612 0.06785340
    ## 71   0.8 0.1139087396 0.7996889 0.5976539 0.03410217 0.06817045
    ## 72   0.8 0.2631439626 0.8613681 0.7162082 0.02625337 0.05470655
    ## 73   0.9 0.0003244162 0.8019519 0.6021646 0.03553674 0.07120470
    ## 74   0.9 0.0007494434 0.8019519 0.6021646 0.03553674 0.07120470
    ## 75   0.9 0.0017313116 0.8013499 0.6009415 0.03522498 0.07056875
    ## 76   0.9 0.0039995543 0.7999827 0.5982096 0.03639993 0.07292394
    ## 77   0.9 0.0092394891 0.7986202 0.5955686 0.03531881 0.07075028
    ## 78   0.9 0.0213444181 0.7988515 0.5962033 0.03512192 0.07023845
    ## 79   0.9 0.0493083742 0.7923390 0.5833426 0.03295516 0.06601385
    ## 80   0.9 0.1139087396 0.7724053 0.5454963 0.03301429 0.06559233
    ## 81   0.9 0.2631439626 0.8613681 0.7162082 0.02625337 0.05470655
    ## 82   1.0 0.0003244162 0.8018010 0.6018558 0.03532890 0.07077902
    ## 83   1.0 0.0007494434 0.8018010 0.6018558 0.03532890 0.07077902
    ## 84   1.0 0.0017313116 0.8010457 0.6003332 0.03549472 0.07109217
    ## 85   1.0 0.0039995543 0.7992263 0.5967474 0.03683886 0.07372767
    ## 86   1.0 0.0092394891 0.7986197 0.5956305 0.03540331 0.07087540
    ## 87   1.0 0.0213444181 0.7966551 0.5919880 0.03521572 0.07031739
    ## 88   1.0 0.0493083742 0.7827188 0.5648233 0.03478974 0.06937438
    ## 89   1.0 0.1139087396 0.7719490 0.5449882 0.03325934 0.06609398
    ## 90   1.0 0.2631439626 0.7159434 0.4066320 0.05815741 0.12447284

``` r
# Highest accuracy = 0.8515188, Kappa = 0.6956381, alpha = 0.7, lambda = 0.2578427450
```

Professor’s solution:

``` r
set.seed(123)

model.2 = train(alc_consumption ~ ., data = train_data, method = "glm", trControl = trainControl("cv", number = 10), preProcess = c("center", "scale"))

model.2$results
```

    ##   parameter  Accuracy    Kappa AccuracySD    KappaSD
    ## 1      none 0.8046378 0.608047 0.04202151 0.08216859

``` r
coef(model.2$finalModel) # Nothing gets shrunk down to 0, but we still get to see variable importance.
```

    ##             (Intercept)       neurotocism_score      extroversion_score 
    ##             0.237918132             0.076151173             0.207968470 
    ##          openness_score     agreeableness_score conscientiousness_score 
    ##             0.070012096             0.108023168             0.009046247 
    ##     impulsiveness_score      sens_seeking_score 
    ##             1.768164799             0.291009760

``` r
confusionMatrix(model.2)
```

    ## Cross-Validated (10 fold) Confusion Matrix 
    ## 
    ## (entries are percentual average cell counts across resamples)
    ##  
    ##                Reference
    ## Prediction      NotCurrentUse CurrentUse
    ##   NotCurrentUse          37.0        9.8
    ##   CurrentUse              9.8       43.5
    ##                             
    ##  Accuracy (average) : 0.8045

``` r
# Average accuracy = 0.8045
```

We can see that although we have a lower overall accuracy and there is
some misclassification of “CurrentUse”, but we are doing much better
with the “NotCurrentUse” prediction. **So there is a balance in
traditional logistic regression.**

### LASSO Model

``` r
set.seed(125)

lambda.2 = 10^seq(-3, 1, length = 100)
lambda.grid = expand.grid(alpha = 1, lambda = lambda.2)

lasso = train(alc_consumption ~ ., data = train_data, method = "glmnet", preProc = c("center", "scale"), trControl = control.settings, tuneGrid = lambda.grid)

# Find best tuned parameters
lasso$bestTune
```

    ##    alpha    lambda
    ## 60     1 0.2420128

``` r
lasso$results
```

    ##     alpha       lambda  Accuracy      Kappa  AccuracySD    KappaSD
    ## 1       1  0.001000000 0.8003762 0.59901339 0.029389016 0.05920839
    ## 2       1  0.001097499 0.8003762 0.59900466 0.029506444 0.05944578
    ## 3       1  0.001204504 0.8004519 0.59915183 0.029640631 0.05972192
    ## 4       1  0.001321941 0.8003004 0.59885659 0.029527540 0.05949009
    ## 5       1  0.001450829 0.8002241 0.59870943 0.029634884 0.05968690
    ## 6       1  0.001592283 0.7998441 0.59795959 0.029614267 0.05963700
    ## 7       1  0.001747528 0.7998441 0.59796122 0.029633543 0.05966186
    ## 8       1  0.001917910 0.7996932 0.59766459 0.029783232 0.05991285
    ## 9       1  0.002104904 0.7996169 0.59752279 0.029864655 0.06007378
    ## 10      1  0.002310130 0.7996926 0.59766429 0.029999947 0.06033178
    ## 11      1  0.002535364 0.7991612 0.59660626 0.029913790 0.06016920
    ## 12      1  0.002782559 0.7987806 0.59586261 0.030038658 0.06041265
    ## 13      1  0.003053856 0.7987806 0.59588658 0.030115179 0.06055949
    ## 14      1  0.003351603 0.7986297 0.59560047 0.030029511 0.06037381
    ## 15      1  0.003678380 0.7984036 0.59515175 0.029918469 0.06014235
    ## 16      1  0.004037017 0.7981011 0.59456123 0.029905878 0.06010653
    ## 17      1  0.004430621 0.7977229 0.59381789 0.029278899 0.05886732
    ## 18      1  0.004862602 0.7974956 0.59336836 0.029047672 0.05839883
    ## 19      1  0.005336699 0.7976477 0.59366951 0.028735148 0.05781171
    ## 20      1  0.005857021 0.7977246 0.59381422 0.028952174 0.05824190
    ## 21      1  0.006428073 0.7977252 0.59383152 0.028845656 0.05802579
    ## 22      1  0.007054802 0.7974233 0.59325160 0.028981006 0.05828296
    ## 23      1  0.007742637 0.7972735 0.59295774 0.028779309 0.05788335
    ## 24      1  0.008497534 0.7969699 0.59234457 0.028351367 0.05706168
    ## 25      1  0.009326033 0.7971977 0.59281457 0.029382365 0.05910541
    ## 26      1  0.010235310 0.7972706 0.59300102 0.029545852 0.05941894
    ## 27      1  0.011233240 0.7975719 0.59361773 0.029397270 0.05914794
    ## 28      1  0.012328467 0.7977212 0.59397626 0.029460107 0.05923879
    ## 29      1  0.013530478 0.7980983 0.59475052 0.029938502 0.06018487
    ## 30      1  0.014849683 0.7980994 0.59478194 0.029070717 0.05844994
    ## 31      1  0.016297508 0.7980237 0.59466019 0.028707614 0.05769777
    ## 32      1  0.017886495 0.7965837 0.59186141 0.028065078 0.05636861
    ## 33      1  0.019630407 0.7962835 0.59126100 0.028407956 0.05710370
    ## 34      1  0.021544347 0.7956797 0.59007638 0.029263431 0.05882346
    ## 35      1  0.023644894 0.7949996 0.58874580 0.029396817 0.05908751
    ## 36      1  0.025950242 0.7940113 0.58676622 0.029298781 0.05895275
    ## 37      1  0.028480359 0.7925668 0.58392786 0.030323176 0.06088960
    ## 38      1  0.031257158 0.7908266 0.58051064 0.030481309 0.06126319
    ## 39      1  0.034304693 0.7890818 0.57706828 0.030385282 0.06097639
    ## 40      1  0.037649358 0.7868819 0.57276743 0.031233107 0.06262430
    ## 41      1  0.041320124 0.7859733 0.57103776 0.031718508 0.06355595
    ## 42      1  0.045348785 0.7852140 0.56956944 0.032058430 0.06421522
    ## 43      1  0.049770236 0.7831737 0.56569187 0.032154517 0.06434247
    ## 44      1  0.054622772 0.7803752 0.56036502 0.032577138 0.06508121
    ## 45      1  0.059948425 0.7779498 0.55580743 0.031235013 0.06237237
    ## 46      1  0.065793322 0.7760587 0.55234002 0.032132734 0.06413767
    ## 47      1  0.072208090 0.7733261 0.54722815 0.031798614 0.06351026
    ## 48      1  0.079248290 0.7718104 0.54445169 0.032501379 0.06491657
    ## 49      1  0.086974900 0.7717312 0.54446170 0.032062066 0.06405944
    ## 50      1  0.095454846 0.7716566 0.54439256 0.032132580 0.06422289
    ## 51      1  0.104761575 0.7719596 0.54501244 0.032231392 0.06442678
    ## 52      1  0.114975700 0.7719596 0.54501244 0.032231392 0.06442678
    ## 53      1  0.126185688 0.7719596 0.54501244 0.032231392 0.06442678
    ## 54      1  0.138488637 0.7719596 0.54501244 0.032231392 0.06442678
    ## 55      1  0.151991108 0.8189163 0.63468180 0.050643501 0.09788747
    ## 56      1  0.166810054 0.8613761 0.71627171 0.026166804 0.05442667
    ## 57      1  0.183073828 0.8613761 0.71627171 0.026166804 0.05442667
    ## 58      1  0.200923300 0.8613761 0.71627171 0.026166804 0.05442667
    ## 59      1  0.220513074 0.8613761 0.71627171 0.026166804 0.05442667
    ## 60      1  0.242012826 0.8613761 0.71627171 0.026166804 0.05442667
    ## 61      1  0.265608778 0.6953573 0.36236494 0.034099199 0.07365513
    ## 62      1  0.291505306 0.5375795 0.01136219 0.007137399 0.01570561
    ## 63      1  0.319926714 0.5325765 0.00000000 0.002587401 0.00000000
    ## 64      1  0.351119173 0.5325765 0.00000000 0.002587401 0.00000000
    ## 65      1  0.385352859 0.5325765 0.00000000 0.002587401 0.00000000
    ## 66      1  0.422924287 0.5325765 0.00000000 0.002587401 0.00000000
    ## 67      1  0.464158883 0.5325765 0.00000000 0.002587401 0.00000000
    ## 68      1  0.509413801 0.5325765 0.00000000 0.002587401 0.00000000
    ## 69      1  0.559081018 0.5325765 0.00000000 0.002587401 0.00000000
    ## 70      1  0.613590727 0.5325765 0.00000000 0.002587401 0.00000000
    ## 71      1  0.673415066 0.5325765 0.00000000 0.002587401 0.00000000
    ## 72      1  0.739072203 0.5325765 0.00000000 0.002587401 0.00000000
    ## 73      1  0.811130831 0.5325765 0.00000000 0.002587401 0.00000000
    ## 74      1  0.890215085 0.5325765 0.00000000 0.002587401 0.00000000
    ## 75      1  0.977009957 0.5325765 0.00000000 0.002587401 0.00000000
    ## 76      1  1.072267222 0.5325765 0.00000000 0.002587401 0.00000000
    ## 77      1  1.176811952 0.5325765 0.00000000 0.002587401 0.00000000
    ## 78      1  1.291549665 0.5325765 0.00000000 0.002587401 0.00000000
    ## 79      1  1.417474163 0.5325765 0.00000000 0.002587401 0.00000000
    ## 80      1  1.555676144 0.5325765 0.00000000 0.002587401 0.00000000
    ## 81      1  1.707352647 0.5325765 0.00000000 0.002587401 0.00000000
    ## 82      1  1.873817423 0.5325765 0.00000000 0.002587401 0.00000000
    ## 83      1  2.056512308 0.5325765 0.00000000 0.002587401 0.00000000
    ## 84      1  2.257019720 0.5325765 0.00000000 0.002587401 0.00000000
    ## 85      1  2.477076356 0.5325765 0.00000000 0.002587401 0.00000000
    ## 86      1  2.718588243 0.5325765 0.00000000 0.002587401 0.00000000
    ## 87      1  2.983647240 0.5325765 0.00000000 0.002587401 0.00000000
    ## 88      1  3.274549163 0.5325765 0.00000000 0.002587401 0.00000000
    ## 89      1  3.593813664 0.5325765 0.00000000 0.002587401 0.00000000
    ## 90      1  3.944206059 0.5325765 0.00000000 0.002587401 0.00000000
    ## 91      1  4.328761281 0.5325765 0.00000000 0.002587401 0.00000000
    ## 92      1  4.750810162 0.5325765 0.00000000 0.002587401 0.00000000
    ## 93      1  5.214008288 0.5325765 0.00000000 0.002587401 0.00000000
    ## 94      1  5.722367659 0.5325765 0.00000000 0.002587401 0.00000000
    ## 95      1  6.280291442 0.5325765 0.00000000 0.002587401 0.00000000
    ## 96      1  6.892612104 0.5325765 0.00000000 0.002587401 0.00000000
    ## 97      1  7.564633276 0.5325765 0.00000000 0.002587401 0.00000000
    ## 98      1  8.302175681 0.5325765 0.00000000 0.002587401 0.00000000
    ## 99      1  9.111627561 0.5325765 0.00000000 0.002587401 0.00000000
    ## 100     1 10.000000000 0.5325765 0.00000000 0.002587401 0.00000000

``` r
# Highest accuracy = 0.8515234, Kappa = 0.6957193637, alpha = 1, lambda = 0.242012826
```

Professor’s Solution:

``` r
set.seed(123)

model.3 = train(alc_consumption ~ ., data = train_data, method = "glmnet", trControl = trainControl("cv", number = 10), preProcess = c("center", "scale"), tuneGrid = expand.grid(alpha = 1, lambda = lambda))

# Output best value of alpha & lambda
model.3$finalModel$tuneValue
```

    ##    alpha   lambda
    ## 40     1 0.231013

``` r
model.3$bestTune
```

    ##    alpha   lambda
    ## 40     1 0.231013

``` r
confusionMatrix(model.3)
```

    ## Cross-Validated (10 fold) Confusion Matrix 
    ## 
    ## (entries are percentual average cell counts across resamples)
    ##  
    ##                Reference
    ## Prediction      NotCurrentUse CurrentUse
    ##   NotCurrentUse          32.9        0.0
    ##   CurrentUse             13.9       53.3
    ##                             
    ##  Accuracy (average) : 0.8614

``` r
coef(model.3$finalModel, model.3$bestTune$lambda)
```

    ## 8 x 1 sparse Matrix of class "dgCMatrix"
    ##                                s1
    ## (Intercept)             0.1335651
    ## neurotocism_score       .        
    ## extroversion_score      .        
    ## openness_score          .        
    ## agreeableness_score     .        
    ## conscientiousness_score .        
    ## impulsiveness_score     0.2992413
    ## sens_seeking_score      .

# 2. Decide final model

Based on the performance tests of all three models, I decided to use the
**LASSO** model because it has the highest accuracy (0.8515234), as well
as the highest Kappa value (0.6957193637), among all three models. The
best tune of the LASSO model is lambda equals 0.242012826.

- The **Kappa** value is a metric that compares an *Observed Accuracy*
  with an *Expected Accuracy* (random chance). The kappa statistic is
  used not only to evaluate a single classifier, but also to evaluate
  classifiers amongst themselves. It is a measurement of the accuracy of
  a model while taking into account chance. The closer the value is to 1
  the better.
  - Cited: rbx (<https://stats.stackexchange.com/users/37276/rbx>),
    Cohen’s kappa in plain English, URL (version: 2017-10-29):
    <https://stats.stackexchange.com/q/82187>

# 3. Apply final model in the test set

I decided to use Confusion Matrix as my final evaluation metric.

``` r
# Test model
test_outcome = predict(lasso, test_data)

# Evaluation metric:
confusionMatrix(test_outcome, test_data$alc_consumption, positive = "CurrentUse")
```

    ## Confusion Matrix and Statistics
    ## 
    ##                Reference
    ## Prediction      NotCurrentUse CurrentUse
    ##   NotCurrentUse           169          0
    ##   CurrentUse               95        301
    ##                                           
    ##                Accuracy : 0.8319          
    ##                  95% CI : (0.7984, 0.8618)
    ##     No Information Rate : 0.5327          
    ##     P-Value [Acc > NIR] : < 2.2e-16       
    ##                                           
    ##                   Kappa : 0.6546          
    ##                                           
    ##  Mcnemar's Test P-Value : < 2.2e-16       
    ##                                           
    ##             Sensitivity : 1.0000          
    ##             Specificity : 0.6402          
    ##          Pos Pred Value : 0.7601          
    ##          Neg Pred Value : 1.0000          
    ##              Prevalence : 0.5327          
    ##          Detection Rate : 0.5327          
    ##    Detection Prevalence : 0.7009          
    ##       Balanced Accuracy : 0.8201          
    ##                                           
    ##        'Positive' Class : CurrentUse      
    ## 

# Report Analysis and Results

Before conducting any analysis, I cleaned the `alcohol_use` dataset and
checked the distribution of the outcome (`alc_consumption`). The
proportion of `*CurrentUse*` is 0.53, while the proportion of
`*NotCurrentUse*` is 0.47. I think they are pretty balance so I decided
not to down sample the outcome during `trainControl`. I partitioned the
dataset into a 70/30 split.

To better predict the current alcohol consumption, I have created and
compared three different models (Elastic Net, Logistic Regression, and
LASSO). Among the three models, LASSO gives the highest accuracy
(0.8515234) and Kappa value (0.6957193637). The best tune lambda of this
model is 0.242012826.

I then fit the LASSO model to the testing dataset to test its
performance. It turned out that this model has an accuracy of 85.5%,
with a sensitivity of 1, a specificity of 0.69, a positive predictive
value of 0.7859, and a negative predictive value of 1.

# Problem 5

Assuming we have a another 2000 participants who had taken the
behavioral test but did not answer whether they currently consume
alcohol or not. We can use this analysis to directly predict the current
alcohol consumption for this group of people using their test scores. On
another instance, if we were interested in seeking the association
between alcohol consumption and liver cancer among this population, this
analysis can indirectly provide information about the prediction of
alcohol consumption of each individual in this population.
